version = 2

[app]
default_provider_id = "opencode"

[app.router]
enabled = true
provider_id = "opencode"
model_id = "gpt-5.2"
min_confidence = 0.7
max_input_chars = 600

[app.web_tools]
enabled = true
max_results = 8
recency = "week"
# allowed_domains and blocked_domains are mutually exclusive.
allowed_domains = []
blocked_domains = []

# Reasoning / Thinking controls (applies only to models flagged as reasoning-capable by modelsdev).
# RemcoChat does NOT stream reasoning text to the UI by default.
[app.reasoning]
enabled = true
effort = "medium" # "minimal" | "low" | "medium" | "high"

# Note: when web tools are enabled and the selected model advertises OpenAI `web_search`,
# RemcoChat will hide the "minimal" option in the UI and will coerce a requested
# "minimal" to "low" server-side (OpenAI rejects web_search with minimal effort).

# WARNING: if enabled, the UI will display model reasoning text.
# Keep this off unless you fully trust all users on your network.
expose_to_client = false

# OpenAI Responses only (optional). Leave empty to disable.
openai_summary = ""

# Anthropic Messages thinking budget tokens (optional). 0/empty disables explicit budget.
anthropic_budget_tokens = 0

# Google Generative AI thinking budget (optional). 0/empty disables explicit budget.
google_thinking_budget = 0

# Attachments (documents only) are enabled by default.
# Document extraction runs in Vercel Sandbox.
# Dependencies:
# - Env must include VERCEL_TOKEN (or VERCEL_API_KEY), VERCEL_TEAM_ID (or VERCEL_ORG_ID), and VERCEL_PROJECT_ID.
# - If credentials are missing, attachment uploads still work but extraction will fail at send-time.
[app.attachments]
enabled = true
allowed_media_types = [
  "text/plain",
  "text/markdown",
  "text/csv",
  "application/json",
  "application/pdf",
]
max_files_per_message = 3
max_file_size_bytes = 2000000
max_total_size_bytes = 5000000
max_extracted_text_chars = 120000
temporary_ttl_ms = 21600000 # 6 hours

[app.attachments.sandbox]
runtime = "node22"
vcpus = 2
timeout_ms = 900000

[app.attachments.processing]
timeout_ms = 30000
max_stdout_chars = 200000
max_stderr_chars = 20000

# Bash tools (Vercel Sandbox) are disabled by default.
# Enabling them requires BOTH:
# - config: app.bash_tools.enabled = true
# - env: REMCOCHAT_ENABLE_BASH_TOOL=1
# Dependencies:
# - access="localhost": tools are available for localhost requests only.
# - access="lan": tools are available ONLY when the request includes the admin token.
#   Set env REMCOCHAT_ADMIN_TOKEN and send it on every /api/chat request via x-remcochat-admin-token.
# - provider="vercel": requires Vercel Sandbox credentials (see attachments section above).
# - provider="docker": requires sandboxd running and docker.orchestrator_url configured.
# - seed.mode="git" requires seed.git_url; seed.mode="upload" requires project_root.
[app.bash_tools]
enabled = false
provider = "vercel"  # "vercel" | "docker"
access = "localhost" # "localhost" | "lan"
project_root = ""    # required if seed.mode="upload"
max_stdout_chars = 12000
max_stderr_chars = 12000
timeout_ms = 30000
max_concurrent_sandboxes = 2
idle_ttl_ms = 900000 # 15 minutes

[app.bash_tools.sandbox]
runtime = "node22" # vercel: "node22" | "python3.13"; docker: "node24" | "python3.13"
ports = [3000] # Vercel: exposed as public URLs; Docker: published via sandboxd (max 4). Use [] to disable.
vcpus = 2
timeout_ms = 900000

[app.bash_tools.docker]
orchestrator_url = "" # required when provider="docker"
# Docker compose (recommended): "http://sandboxd:8080" (sandboxd private to compose network)
# Host dev (running sandboxd locally): "http://127.0.0.1:8080"
admin_token_env = "REMCOCHAT_ADMIN_TOKEN" # env var to use for sandboxd auth
network_mode = "default" # "default" (egress allowed) | "none"
memory_mb = 2048

[app.bash_tools.seed]
mode = "git"         # "git" | "upload"
git_url = ""         # required if mode="git"
git_revision = ""    # optional
upload_include = "**/*" # used only for mode="upload"

# Hue Gateway API v2 integration (optional).
#
# This enables the server-side `hueGateway` tool (preferred) which calls the Hue Gateway directly
# using typed `/v2/actions` requests (no bash/curl).
#
# Auth values are read from environment variables at runtime; do not put secrets in config.toml.
[app.hue_gateway]
enabled = false
access = "localhost" # "localhost" (localhost requests only) | "lan" (requires admin policy / token)
base_urls = ["http://hue-gateway:8000", "http://host.docker.internal:8000", "http://localhost:8000"]
timeout_ms = 8000

# Env var NAMES (not secret values):
auth_header_env = "HUE_AUTH_HEADER" # full header line (e.g. "Authorization: Bearer ...")
bearer_token_env = "HUE_TOKEN"      # used when auth_header_env is unset
api_key_env = "HUE_API_KEY"         # used when auth_header_env is unset (preferred over bearer)

# Providers configure only:
# - how to connect (base_url + api_key_env)
# - which models are allowed (allowed_model_ids + default_model_id)
# Everything else (labels/capabilities/types/limits/modalities) is resolved at runtime
# via the `modelsdev` CLI and stored in-memory until server restart.

# [providers.vercel]
# name = "Vercel AI Gateway"
# api_key_env = "VERCEL_AI_GATEWAY_API_KEY"
# base_url = "https://ai-gateway.vercel.sh/v3/ai"
# modelsdev_provider_id = "vercel"
# default_model_id = "openai/gpt-5.2-chat"
# allowed_model_ids = [
#   "openai/gpt-5.2-chat",
#   "openai/gpt-4o-mini",
#   "openai/gpt-5.2-pro",
#   "openai/gpt-5.2-codex",
#   "anthropic/claude-opus-4.5",
# ]

[providers.opencode]
name = "OpenCode Zen"
api_key_env = "OPENCODE_API_KEY"
base_url = "https://opencode.ai/zen/v1"
modelsdev_provider_id = "opencode"
default_model_id = "gpt-5.2"
allowed_model_ids = [
  "gpt-5-nano",
  "gpt-5.2",
  "gpt-5.2-codex",
  "claude-opus-4-5",
  "glm-4.7-free",
  "kimi-k2",
]
