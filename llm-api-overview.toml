# llm-api-overview.toml
# Purpose: Common LLM provider API interaction patterns, plus mapping to Vercel AI SDK v6 primitives.
# Note: This file also includes a "model API type" matrix (e.g. openai_response, openai_completions, anthropic_messages).
# Generated: 2026-01-15

[meta]
title = "LLM API Types Overview (incl. Vercel AI SDK v6 mapping)"
version = "1.1"
generated_at = "2026-01-15"
language = "nl"

references = [
  "https://vercel.com/blog/ai-sdk-6",
  "https://ai-sdk.dev/docs/announcing-ai-sdk-6-beta",
  "https://ai-sdk.dev/providers/ai-sdk-providers/openai",
  "https://ai-sdk.dev/providers/ai-sdk-providers/anthropic",
  "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text",
  "https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text",
  "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
  "https://ai-sdk.dev/docs/reference/ai-sdk-core/embed",
  "https://ai-sdk.dev/docs/reference/ai-sdk-core/rerank",
]

[vercel_ai_sdk_v6]
core_package = "ai"
provider_packages_examples = ["@ai-sdk/openai", "@ai-sdk/anthropic", "@ai-sdk/cohere"]
key_primitives = [
  "generateText()",
  "streamText()",
  "Output.object() (structured output via generateText/streamText)",
  "tools + stopWhen (multi-step tool calling loop)",
  "Agent class (higher-level agent abstraction)",
  "embed()/embedMany() (embeddings)",
  "rerank() (reranking models)",
]
provider_routing_note = "AI SDK is provider-agnostic. Underlying provider API families are chosen by the provider package (or by explicit factory selection, e.g. openai.responses/openai.chat/openai.completion)."
structured_output_note = "generateObject()/streamObject() exist but are deprecated in favor of generateText/streamText with Output.* in v6 docs."

# -----------------------------------------------------------------------------
# Provider-level model API types (the 'API family' a model call goes through)
# -----------------------------------------------------------------------------

[[model_api_types]]
id = "openai_response"
provider = "openai"
kind = "language_model"
api_family = "responses"
http_endpoint = "/v1/responses"
ai_sdk_package = "@ai-sdk/openai"
ai_sdk_factory = "openai('gpt-5') OR openai.responses('gpt-5')"
notes = [
  "Default API family for the OpenAI provider since AI SDK 5.",
  "Best fit when you want the modern OpenAI stack (tools/structured/multimodal, model-dependent).",
]

[[model_api_types]]
id = "openai_chat_completions"
provider = "openai"
kind = "language_model"
api_family = "chat_completions"
http_endpoint = "/v1/chat/completions"
ai_sdk_package = "@ai-sdk/openai"
ai_sdk_factory = "openai.chat('gpt-5')"
notes = [
  "Explicitly selects the Chat Completions API family.",
]

[[model_api_types]]
id = "openai_completions"
provider = "openai"
kind = "language_model"
api_family = "completions"
http_endpoint = "/v1/completions"
ai_sdk_package = "@ai-sdk/openai"
ai_sdk_factory = "openai.completion('gpt-3.5-turbo-instruct')"
notes = [
  "Legacy-style completion API family; in AI SDK docs currently only gpt-3.5-turbo-instruct is supported via .completion().",
]

[[model_api_types]]
id = "openai_embeddings"
provider = "openai"
kind = "embedding_model"
api_family = "embeddings"
http_endpoint = "/v1/embeddings"
ai_sdk_package = "@ai-sdk/openai"
ai_sdk_factory = "openai.embedding('text-embedding-3-large')"
notes = [
  "Used with embed()/embedMany().",
]

[[model_api_types]]
id = "anthropic_messages"
provider = "anthropic"
kind = "language_model"
api_family = "messages"
http_endpoint = "/v1/messages"
ai_sdk_package = "@ai-sdk/anthropic"
ai_sdk_factory = "anthropic('claude-3-haiku-20240307')"
notes = [
  "Anthropic provider uses the Messages API family.",
]

# -----------------------------------------------------------------------------
# API interaction types (conceptual patterns), mapped to AI SDK v6 + model API types
# -----------------------------------------------------------------------------

[[api_types]]
id = "completion_api"
name = "Completion (prompt -> tekst)"
provider_level = "Single prompt string, stateless; output is plain text."
typical_provider_endpoints = ["completions", "responses (text)"]
typical_use_cases = ["samenvatten", "herformuleren", "classificatie", "batch-achtige offline taken"]
operational_notes = [
  "Je beheert context zelf: alles wat het model moet weten stuur je mee per request.",
  "Goed voor eenvoudige pipelines zonder chatgeschiedenis.",
]
[api_types.model_api_matrix]
openai = ["openai_completions", "openai_response"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
sync = "generateText({ model, prompt })"
streaming = "streamText({ model, prompt })"
structured_output = "generateText({ ..., output: Output.object({ schema }) })"
tools = "generateText/streamText({ tools, stopWhen })"

[[api_types]]
id = "chat_messages_api"
name = "Chat / Messages (conversation -> response)"
provider_level = "Array van messages met rollen; conversational context in de request."
typical_provider_endpoints = ["chat completions", "responses (messages)", "anthropic messages"]
typical_use_cases = ["chatbots", "assistants", "multi-turn flows", "system prompts + beleid"]
operational_notes = [
  "Ook hier blijft context window management jouw verantwoordelijkheid (truncation/samenvatting).",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
sync = "generateText({ model, messages })"
streaming = "streamText({ model, messages })"
memory_pattern = "Sla messages extern op (db/kv) en stuur relevante history mee; of gebruik Agent abstractions."
agent_option = "new Agent({ model, instructions, tools, ... })"

[[api_types]]
id = "streaming_api"
name = "Streaming (incremental output delivery)"
provider_level = "Output komt token-/chunk-gewijs binnen (SSE/WebSocket afhankelijk van provider)."
typical_provider_endpoints = ["stream=true varianten van chat/completions/responses"]
typical_use_cases = ["chat UI", "lage perceived latency", "progressieve rendering"]
operational_notes = [
  "Vereist client-side stream verwerking en cancel/retry logica.",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions", "openai_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
core = "streamText()"
ui_helpers = "AI SDK UI hooks (bv. useChat) kunnen server streaming consumeren (framework afhankelijk)."

[[api_types]]
id = "tool_function_calling_api"
name = "Tool / Function calling (LLM -> gestructureerde call)"
provider_level = "Model retourneert (toolName, arguments) om door jouw app te executen; vaak JSON schema gestuurd."
typical_provider_endpoints = ["tools/function calling", "tool calls in responses API's"]
typical_use_cases = ["workflow orchestration", "agentic flows", "API integraties", "DB queries"]
operational_notes = [
  "Altijd argument-validatie en autorisatie toepassen; behandel tool input als onbetrouwbaar.",
  "Multi-step tool loops (tool -> result -> vervolgprompt) zijn essentieel voor agentic gedrag.",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
core = "generateText/streamText({ tools })"
multi_step = "stopWhen + tool results (SDK voert tool-call loop uit totdat stop-conditie geldt)"

[[api_types]]
id = "structured_output_api"
name = "Structured output (schema-constrained JSON)"
provider_level = "Model output moet valide JSON/object conform schema zijn (extractie, classificatie, forms)."
typical_provider_endpoints = ["Structured Outputs / JSON mode", "tool-calling + structured return"]
typical_use_cases = ["information extraction", "ETL", "form parsing", "typed outputs voor backends"]
operational_notes = [
  "Valideer output altijd (schema) en ontwerp schema's compact om foutkans te beperken.",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
preferred = "generateText/streamText({ output: Output.object({ schema }) })"
deprecation_note = "generateObject()/streamObject() zijn deprecated; Output.* is de aanbevolen route."

[[api_types]]
id = "embeddings_api"
name = "Embeddings (tekst -> vector)"
provider_level = "Geen tekstgeneratie; output is een vector (number[])."
typical_provider_endpoints = ["embeddings"]
typical_use_cases = ["RAG retrieval", "semantische search", "clustering", "deduplicatie"]
operational_notes = [
  "Gebruik een vector index/database; beheer dimensionaliteit en normalisatie consistent.",
]
[api_types.model_api_matrix]
openai = ["openai_embeddings"]
[api_types.vercel_ai_sdk_v6]
single = "embed({ model, value })"
batch = "embedMany({ model, values })"

[[api_types]]
id = "reranking_api"
name = "Reranking (query + kandidaten -> herordening)"
provider_level = "Model rangschikt documenten/kandidaten op relevantie; vaak gebruikt na vector search."
typical_provider_endpoints = ["rerank / reranking models (provider-specifiek)"]
typical_use_cases = ["betere top-k context voor RAG", "search relevance", "triage van resultaten"]
operational_notes = [
  "Rerank pas toe op een beperkt kandidaat-set (bv. top 20-200) na retrieval om kosten te beperken.",
]
[api_types.vercel_ai_sdk_v6]
core = "rerank({ model, query, documents, topN })"
model_factory_examples = "bv. cohere.reranking('rerank-v3.5') (provider-specifiek)"

[[api_types]]
id = "multimodal_api"
name = "Multimodal (text + image/audio -> text/object)"
provider_level = "Input/Output kan meerdere modaliteiten bevatten (bijv. image prompt, audio transcriptie)."
typical_provider_endpoints = ["responses multimodal", "vision/chat", "audio transcription", "speech synthesis"]
typical_use_cases = ["vision Q&A", "document understanding", "transcriptie", "spraak output"]
operational_notes = [
  "Let op grotere payloads, latency en privacy; voorkom dat je onnodig binaire data logt.",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
vision_text = "generateText({ messages: [{ content: [{ type: 'text' }, { type: 'image' }] }] })"
audio_note = "Audio endpoints zijn provider-/model-afhankelijk; AI SDK biedt aparte helpers bij sommige providers."

[[api_types]]
id = "agent_abstraction_api"
name = "Agent abstraction (loop + tools + policy)"
provider_level = "Hoger niveau orchestratie bovenop chat/tools; beheert loops en configuratie herbruikbaar."
typical_provider_endpoints = ["(geen aparte provider endpoint; meestal bovenop LLM + tools)"]
typical_use_cases = ["autonome taken", "multi-tool workflows", "research agents", "ops runbooks"]
operational_notes = [
  "Agent frameworks verbergen vaak complexiteit; behoud observability (logs/traces) en grenzen.",
]
[api_types.model_api_matrix]
openai = ["openai_response", "openai_chat_completions"]
anthropic = ["anthropic_messages"]
[api_types.vercel_ai_sdk_v6]
recommended = "Agent class (AI SDK 6) voor herbruikbare agents"
low_level_alternative = "generateText/streamText + stopWhen voor maximale controle"

[[api_types]]
id = "batch_async_provider_api"
name = "Batch/Async processing (provider-side jobs)"
provider_level = "Asynchroon job model bij providers (bulk requests, goedkoper, langere turnaround)."
typical_provider_endpoints = ["provider batch APIs (verschilt per provider)"]
typical_use_cases = ["offline evaluaties", "massale documentverwerking", "nachtjobs"]
operational_notes = [
  "Niet elke provider ondersteunt dit, en semantiek verschilt per provider.",
  "AI SDK Core heeft geen uniforme 'batch LLM jobs' primitive; je gebruikt provider batch endpoints of bouwt zelf een queue/worker met parallel generateText calls.",
]
[api_types.vercel_ai_sdk_v6]
embedding_batch = "embedMany()"
llm_batch_jobs = "Provider-specifiek of eigen queue/worker aanpak."
